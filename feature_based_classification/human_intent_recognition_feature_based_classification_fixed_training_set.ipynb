{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force-based human's intent recognition: Feature-based classification\n",
    "We propose a twofold machine learning approach to infer the human operator's intentions by means of force signals. First, we reduce the dimensionality of the data using an unsupervised method: Gaussian Process Latent Variable Model (GPLVM)[1]. Then, we use a Support Vector Machine (SVM) classifier which is trained using the lower dimensional representation of the data.\n",
    "GPLVM is a non-linear dimensionality reduction method which can be considered as a multiple-output GP regression model where only the output data are given. The inputs are unobserved and treated as latent variables, however, instead of integrating out the latent variables, they are optimised. By doing this, the model gets more tractable and some theoretical grounding for the approach is given by the fact that the model can be seen as a non-linear extension of the linear probabilistic PCA (PPCA)[2]. Note that in this case, the temporal sequences are just considered as long feature vectors so that it is not explicitly considered the temporal relation between subsequent signal measurements.\n",
    "\n",
    "### Implementation details\n",
    "The implementation of the proposed method, GPLVM+SVM, relies on two existing libraries: *GPy* library for the dimensionality reduction and the *scikit learn* library for the SVM classifier. In the case of the latter, we used the default values for all the parameters. However, with regard to GPLVM, it has been necessary to set some parameters: kernel, optimiser and the maximum number of optimisation steps. \n",
    "Firstly, we chose a kernel which is a combination of the Radial Basis Function (RBF) kernel together with a *bias* kernel. RBF kernel was selected because it is one of the most well known kernels for non-linear problems. We added the \\textit{bias} kernel to enable the kernel function to be computed not only in the origin of coordinates.\n",
    "\n",
    "Secondly, for the optimisation process, we have used one of the optimisers already implemented in *GPy*, limited-memory Broyden–Fletcher–Goldfarb–Shanno (BFGS). We chose this optimiser because, unlike others included in the library, it was quite stable with respect to the number of optimisation steps needed to converge. \n",
    "Finally, the maximum number of optimisation steps is set to 5000, which in most of the cases was enough for the optimisation to converge. \n",
    "\n",
    "The implementation of the GPLVM algorithm allows to use two different types of latent variable inference: with optimisation step (GPLVM-op) and without optimisation step (GPLVM). For us, the most relevant difference between them is that the inference with optimisation takes more time, but it would be more correct in theory and it would lead to more accurate results.\n",
    "\n",
    "\n",
    "#### Note\n",
    "In this notebook, you can train your classifier with a pre-defined training dataset and generate a final model for your demos. Please, if you want to perform cross validation without replacement and evaluate the performance of the model, go to the another notebook of this folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "from scripts.utils_general import *\n",
    "from scripts.utils_evaluation import *\n",
    "from scripts.utils_visualization import *\n",
    "from scripts.utils_data_process import *\n",
    "from scripts.utils_evaluation_global_variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for processing the data\n",
    "type_of_dataset = 'natural' # natural or mechanical\n",
    "labels = {0:'grab', 1:'move', 2:'polish'}\n",
    "dataset_folder = '../data/'\n",
    "\n",
    "training_portion = 0.75\n",
    "\n",
    "# parameters for data length\n",
    "number_of_measurements = 350 # size of the window\n",
    "step = 1 # for subsampling\n",
    "\n",
    "\n",
    "# GPLVM parameters\n",
    "latent_dimensionality = 10\n",
    "max_iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = read_dataset_(dataset_folder, type_of_dataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loop_evaluations = 1\n",
    "training_samples = np.load('../utils/fixed_training_samples.npy')\n",
    "\n",
    "for i in range (0, loop_evaluations):\n",
    "    \n",
    "    \n",
    "    data_training, data_test = pick_fixed_training_dataset_(processed_data, training_samples, \\\n",
    "                                                              number_of_measurements, step, normalize=False)\n",
    "    # gplvm\n",
    "    gplvm = dualPPCA_init_and_optimization_(type_='gplvm', latent_dimensionality=latent_dimensionality, \\\n",
    "                                            data_training=data_training, max_iterations=max_iterations)\n",
    "    \n",
    "    inferred_gplvm_nop, gplvm_nop_results = dualPPCA_inference_(gplvm, data_test, gplvm_nop_results, optimize_=False)\n",
    "    inferred_gplvm_op, gplvm_op_results = dualPPCA_inference_(gplvm, data_test, gplvm_op_results, optimize_=True)\n",
    "    \n",
    "    gplvm_classifier_init = SVC(gamma='auto', probability=True)\n",
    "    \n",
    "    gplvm_classifier = training_classifier_lower_dim_space_dualPPCA_(gplvm, gplvm_classifier_init, data_training)\n",
    "    \n",
    "    \n",
    "    predictions_gplvm_nop = predict_with_classifier_lower_dim_space_dualPPCA_(gplvm, gplvm_classifier, data_test, \\\n",
    "                                                                          inferred_gplvm_nop)\n",
    "    predictions_gplvm_op = predict_with_classifier_lower_dim_space_dualPPCA_(gplvm, gplvm_classifier, data_test, \\\n",
    "                                                                          inferred_gplvm_op)\n",
    "    \n",
    "    gplvm_nop_results = evaluate_classification_performance_dualPPCA(data_test['test_labels'], \\\n",
    "                                                                     predictions_gplvm_nop, gplvm_nop_results)\n",
    "    gplvm_op_results = evaluate_classification_performance_dualPPCA(data_test['test_labels'], \\\n",
    "                                                                    predictions_gplvm_op, gplvm_op_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplvm_op_mean = dict()\n",
    "gplvm_op_std = dict()\n",
    "gplvm_nop_mean = dict()\n",
    "gplvm_nop_std = dict()\n",
    "\n",
    "for key, value in gplvm_op_results.items():\n",
    "    if key == 'confusion_matrix':\n",
    "        gplvm_op_mean[key] = np.mean(value, axis=0)\n",
    "        gplvm_op_std[key] = np.std(value, axis=0)\n",
    "    else: \n",
    "        gplvm_op_mean[key] = np.mean(value)\n",
    "        gplvm_op_std[key] = np.std(value)\n",
    "for key, value in gplvm_nop_results.items():\n",
    "    if key == 'confusion_matrix':\n",
    "        gplvm_nop_mean[key] = np.mean(value, axis=0)\n",
    "        gplvm_nop_std[key] = np.std(value, axis=0)\n",
    "    else: \n",
    "        gplvm_nop_mean[key] = np.mean(value)\n",
    "        gplvm_nop_std[key] = np.std(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_name = '_latent_' # string containing the variable we are tuning\n",
    "\n",
    "all_results['gplvm_opt_mean'+add_to_name+str(latent_dimensionality)] = gplvm_op_mean\n",
    "all_results['gplvm_opt_std'+add_to_name+str(latent_dimensionality)] = gplvm_op_std\n",
    "all_results['gplvm_no_opt_mean'+add_to_name+str(latent_dimensionality)] = gplvm_nop_mean\n",
    "all_results['gplvm_no_opt_std'+add_to_name+str(latent_dimensionality)] = gplvm_nop_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(gplvm, data_training, \\\n",
    "           'gplvm_window_size'+str(number_of_measurements)+'_latent'+str(latent_dimensionality), 'models/', \"\")\n",
    "\n",
    "save_obj(gplvm_classifier, 'models/', 'gplvm_classifier_latent'+str(latent_dimensionality))\n",
    "\n",
    "load_gplvm = load_model('gplvm', 'gplvm_window_size'+str(number_of_measurements)+'_latent'+str(latent_dimensionality), 'models/', \"\")\n",
    "load_gplvm_classifier = load_obj('models/', 'gplvm_classifier_latent'+str(latent_dimensionality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful tools \n",
    "In the next cells, you can find interesting tools and functionalities of GPy and also some implemented in our work. You could analyse which are the most discriminative latent variables or visualise the distribution of the data in 3D, among others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot to visualise the relevance of each latent variable\n",
    "gplvm.kern.plot_ARD(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric value regarding to the relevance of each latent variable\n",
    "gplvm.input_sensitivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of the three most significant latent variables\n",
    "list(gplvm.get_most_significant_input_dimensions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualisation of the distribution of the training data once the GPLVM has been optimised \n",
    "\n",
    "scatter_color = ['#D62728','#7F7F7F','#1FBECF']\n",
    "plot_name = 'gplvm_inference'\n",
    "y_training = data_training['training_labels']\n",
    "y_test = data_test['test_labels']\n",
    "\n",
    "fig_gplvm_3D = embedded_variables_3D_plot_(model=gplvm, scatter_color=scatter_color, labels=labels, \\\n",
    "                                            y_training=data_training['training_labels'], \\\n",
    "                                            latent_variables=list(gplvm.get_most_significant_input_dimensions()))\n",
    "\n",
    "#py.iplot(fig_gplvm_3D, filename=plot_name) # export to plotly server (limit of 100 each 24h)\n",
    "plot(fig_gplvm_3D) # offline approach (unlimited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3D visualisation of the distribution of the data (training + test/inferred) once the GPLVM has been optimised \n",
    "\n",
    "scatter_color = ['#D62728','#7F7F7F','#1FBECF']\n",
    "plot_name = 'gplvm_inference'\n",
    "y_training = data_training['training_labels']\n",
    "y_test = data_test['test_labels']\n",
    "\n",
    "fig_gplvm_3D_inf = embedded_variables_3D_plot_(model=gplvm, scatter_color=scatter_color, \\\n",
    "                                        inferred=inferred_gplvm_op, labels=labels, \\\n",
    "                                        y_training=data_training['training_labels'], \\\n",
    "                                        y_test=data_test['test_labels'])\n",
    "\n",
    "#py.iplot(fig_gplvm, filename=plot_name) # export to plotly server (limit of 100 each 24h)\n",
    "plot(fig_gplvm_3D_inf) # offline approach (unlimited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Lawrence, N. D. (2004). Gaussian process latent variable models for visualisation of high dimensional data. In Advances in neural information processing systems (pp. 329-336).\n",
    "\n",
    "[2] Tipping, M. E., & Bishop, C. M. (1999). Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
