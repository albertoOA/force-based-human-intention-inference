{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force-based human's intent recognition: Feature-based classification\n",
    "We propose a twofold machine learning approach to infer the human operator's intentions by means of force signals. First, we reduce the dimensionality of the data using an unsupervised method: Gaussian Process Latent Variable Model (GPLVM)[1]. Then, we use a Support Vector Machine (SVM) classifier which is trained using the lower dimensional representation of the data.\n",
    "GPLVM is a non-linear dimensionality reduction method which can be considered as a multiple-output GP regression model where only the output data are given. The inputs are unobserved and treated as latent variables, however, instead of integrating out the latent variables, they are optimised. By doing this, the model gets more tractable and some theoretical grounding for the approach is given by the fact that the model can be seen as a non-linear extension of the linear probabilistic PCA (PPCA)[2]. Note that in this case, the temporal sequences are just considered as long feature vectors so that it is not explicitly considered the temporal relation between subsequent signal measurements.\n",
    "\n",
    "### Implementation details\n",
    "The implementation of the proposed method, GPLVM+SVM, relies on two existing libraries: *GPy* library for the dimensionality reduction and the *scikit learn* library for the SVM classifier. In the case of the latter, we used the default values for all the parameters. However, with regard to GPLVM, it has been necessary to set some parameters: kernel, optimiser and the maximum number of optimisation steps. \n",
    "Firstly, we chose a kernel which is a combination of the Radial Basis Function (RBF) kernel together with a *bias* kernel. RBF kernel was selected because it is one of the most well known kernels for non-linear problems. We added the \\textit{bias} kernel to enable the kernel function to be computed not only in the origin of coordinates.\n",
    "\n",
    "Secondly, for the optimisation process, we have used one of the optimisers already implemented in *GPy*, limited-memory Broyden–Fletcher–Goldfarb–Shanno (BFGS). We chose this optimiser because, unlike others included in the library, it was quite stable with respect to the number of optimisation steps needed to converge. \n",
    "Finally, the maximum number of optimisation steps is set to 5000, which in most of the cases was enough for the optimisation to converge. \n",
    "\n",
    "The implementation of the GPLVM algorithm allows to use two different types of latent variable inference: with optimisation step (GPLVM-op) and without optimisation step (GPLVM). For us, the most relevant difference between them is that the inference with optimisation takes more time, but it would be more correct in theory and it would lead to more accurate results.\n",
    "\n",
    "\n",
    "#### Note\n",
    "In this notebook, you can train your classifier with cross validation without replacement and evaluate the results. Please, if you want to do the training with a pre-defined dataset and generate a final model, go to the another notebook of this folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "from scripts.utils_general import *\n",
    "from scripts.utils_evaluation import *\n",
    "from scripts.utils_visualization import *\n",
    "from scripts.utils_data_process import *\n",
    "from scripts.utils_evaluation_global_variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for processing the data\n",
    "type_of_dataset = 'natural' # natural or mechanical\n",
    "labels = {0:'grab', 1:'move', 2:'polish'}\n",
    "dataset_folder = '../data/'\n",
    "\n",
    "training_portion = 0.75\n",
    "cross_validation_iterations = 1\n",
    "\n",
    "\n",
    "# parameters for data length\n",
    "number_of_measurements = 350 # size of the window\n",
    "step = 1 # for subsampling\n",
    "\n",
    "\n",
    "# GPLVM parameters\n",
    "latent_dimensionality = 10\n",
    "max_iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = read_dataset_(dataset_folder, type_of_dataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_training_ids_list = list()\n",
    "\n",
    "for i in range (0, cross_validation_iterations):\n",
    "    data_training, data_test = pick_training_dataset_randomly_(processed_data, training_portion, \\\n",
    "                                                              number_of_measurements, step, normalize=False)\n",
    "    \n",
    "    data_training_ids_list.append(data_training['sample_ids_training'])\n",
    "    \n",
    "    \n",
    "    # gplvm\n",
    "    gplvm = dualPPCA_init_and_optimization_(type_='gplvm', latent_dimensionality=latent_dimensionality, \\\n",
    "                                            data_training=data_training, max_iterations=max_iterations)\n",
    "    \n",
    "    inferred_gplvm_nop, gplvm_nop_results = dualPPCA_inference_(gplvm, data_test, gplvm_nop_results, optimize_=False)\n",
    "    inferred_gplvm_op, gplvm_op_results = dualPPCA_inference_(gplvm, data_test, gplvm_op_results, optimize_=True)\n",
    "    \n",
    "    gplvm_classifier_init = SVC(gamma='auto', probability=True)\n",
    "    \n",
    "    gplvm_classifier = training_classifier_lower_dim_space_dualPPCA_(gplvm, gplvm_classifier_init, data_training)\n",
    "    \n",
    "    \n",
    "    predictions_gplvm_nop = predict_with_classifier_lower_dim_space_dualPPCA_(gplvm, gplvm_classifier, data_test, \\\n",
    "                                                                          inferred_gplvm_nop)\n",
    "    predictions_gplvm_op = predict_with_classifier_lower_dim_space_dualPPCA_(gplvm, gplvm_classifier, data_test, \\\n",
    "                                                                          inferred_gplvm_op)\n",
    "    \n",
    "    gplvm_nop_results = evaluate_classification_performance_dualPPCA(data_test['test_labels'], \\\n",
    "                                                                     predictions_gplvm_nop, gplvm_nop_results)\n",
    "    gplvm_op_results = evaluate_classification_performance_dualPPCA(data_test['test_labels'], \\\n",
    "                                                                    predictions_gplvm_op, gplvm_op_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplvm_op_mean = dict()\n",
    "gplvm_op_std = dict()\n",
    "gplvm_nop_mean = dict()\n",
    "gplvm_nop_std = dict()\n",
    "\n",
    "for key, value in gplvm_op_results.items():\n",
    "    if key == 'confusion_matrix':\n",
    "        gplvm_op_mean[key] = np.mean(value, axis=0)\n",
    "        gplvm_op_std[key] = np.std(value, axis=0)\n",
    "    else: \n",
    "        gplvm_op_mean[key] = np.mean(value)\n",
    "        gplvm_op_std[key] = np.std(value)\n",
    "for key, value in gplvm_nop_results.items():\n",
    "    if key == 'confusion_matrix':\n",
    "        gplvm_nop_mean[key] = np.mean(value, axis=0)\n",
    "        gplvm_nop_std[key] = np.std(value, axis=0)\n",
    "    else: \n",
    "        gplvm_nop_mean[key] = np.mean(value)\n",
    "        gplvm_nop_std[key] = np.std(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_name = '_latent_' # string containing the variable we are tuning\n",
    "\n",
    "all_results['gplvm_opt_mean'+add_to_name+str(latent_dimensionality)] = gplvm_op_mean\n",
    "all_results['gplvm_opt_std'+add_to_name+str(latent_dimensionality)] = gplvm_op_std\n",
    "all_results['gplvm_no_opt_mean'+add_to_name+str(latent_dimensionality)] = gplvm_nop_mean\n",
    "all_results['gplvm_no_opt_std'+add_to_name+str(latent_dimensionality)] = gplvm_nop_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Lawrence, N. D. (2004). Gaussian process latent variable models for visualisation of high dimensional data. In Advances in neural information processing systems (pp. 329-336).\n",
    "\n",
    "[2] Tipping, M. E., & Bishop, C. M. (1999). Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
